d_model: 768
num_heads: 12
d_ff: 3072
num_layers: 12
vocab_size: 50257
context_length: 1024
theta: 10000.0

batch_size: 64
max_iters: 100000
eval_interval: 500
eval_iters: 100
log_interval: 100
checkpoint_interval: 1000
seed: 42

optimizer: adamw
learning_rate: 0.0006
min_lr: 0.00006
weight_decay: 0.01
beta1: 0.9
beta2: 0.999
gradient_clip_norm: 1.0

warmup_iters: 2000
cosine_iters: 100000

train_data: data/tinystories_train_tokens.bin
val_data: data/tinystories_val_tokens.bin
data_dtype: uint16
checkpoint_dir: checkpoints/gpt2-small-tinystories
resume_from: null

use_wandb: false
wandb_project: cs336-assignment1
wandb_run_name: null

device: cuda
compile: false

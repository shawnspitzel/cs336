program: assignment1-basics/cs336_basics/training/pretrain.py
project: AtlasLM-Pretrain
method: grid
metric:
  name: val/loss
  goal: minimize

command:
  - ${env}
  - ${interpreter}
  - ${program}
  - ${args}

parameters:
  # GPT-2 "Tiny" - A smaller variant for faster CPU training
  # ~40M parameters - 3x larger than your current model but trainable on CPU

  d_model:
    value: 512
  num_heads:
    value: 8
  d_ff:
    value: 2048  # 4 * d_model
  num_layers:
    value: 8
  vocab_size:
    value: 50257  # Match GPT-2 vocab
  context_length:
    value: 512  # Reduced from 1024 to save memory
  theta:
    value: 10000.0

  # Training hyperparameters
  batch_size:
    value: 8  # Can handle larger batch than full GPT-2
  max_iters:
    value: 15000
  eval_interval:
    value: 100
  eval_iters:
    value: 20
  log_interval:
    value: 10
  checkpoint_interval:
    value: 500
  seed:
    value: 42

  # Optimizer
  optimizer:
    value: adamw
  learning_rate:
    value: 3.0e-4  # Slightly higher for smaller model
  min_lr:
    value: 3.0e-5
  weight_decay:
    value: 0.01
  beta1:
    value: 0.9
  beta2:
    value: 0.999
  gradient_clip_norm:
    value: 1.0

  # Learning rate schedule
  warmup_iters:
    value: 500
  cosine_iters:
    value: 15000

  # Data paths
  train_data:
    value: /Users/shawnspitzel/cs336/cs326/assignment1-basics/cs336_basics/data/tokenized/tinystories/train/TinyStoriesV2-GPT4-train_tokens_.bin
  val_data:
    value: /Users/shawnspitzel/cs336/cs326/assignment1-basics/cs336_basics/data/tokenized/tinystories/val/TinyStoriesV2-GPT4-valid_val_tokens_.bin
  data_dtype:
    value: uint16

  checkpoint_dir:
    value: /Users/shawnspitzel/cs336/cs326/assignment1-basics/cs336_basics/checkpoints/model/gpt2-tiny-tinystories

  resume_from:
    value: null

  device:
    value: cpu
  compile:
    value: false

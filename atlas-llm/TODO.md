- **Implement optimizer state sharding** ðŸš§
- Profile current implementation
    - Naive Attention vs FlashAttention
    - Compile vs No-Compile
    - Peak memory usage
    - Peak GPU usage
    - Inference bottlenecks
    - Accounting
- Implement FlashAttention2 (from-scratch)
- Implement non-naive distributed training pipeline
- Implement Fully-Sharded Data Parallelism (FSDP)
    - More information at HuggingFace Ultra-Scale Playbook
- Implement Tensor Parallelism
- Quantize model to use F16 mixed-precision data types (torch.autocast())

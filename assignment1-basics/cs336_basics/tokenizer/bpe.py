import regex as re
import warnings
import pickle
from pathlib import Path
from multiprocessing import Pool, cpu_count
from collections import Counter

def _rebuild_word(args):
    """Worker function for parallel corpus rebuilding."""
    word, merge_pair, merged_id = args
    if len(word) < 2:
        return word
    new_word = []
    i = 0
    word_len = len(word)
    while i < word_len:
        if i < word_len - 1 and word[i] == merge_pair[0] and word[i+1] == merge_pair[1]:
            new_word.append(merged_id)
            i += 2
        else:
            new_word.append(word[i])
            i += 1
    return new_word


def _count_pairs_chunk(args):
    """Worker function for parallel pair frequency counting."""
    word_chunk = args
    pairs = []
    for word in word_chunk:
        pairs.extend(zip(word, word[1:]))
    return Counter(pairs)


class BPETokenizer:
    def __init__(self, special_tokens: list[str] = [], num_workers: int = 1):
        self.special_tokens = special_tokens
        self.num_workers = num_workers if num_workers > 0 else cpu_count()
        self.vocabulary: dict[bytes, int] = {}
        self.reverseVocab: dict[int, bytes] = {}
        self.corpus: None | str = None
        self.merges: list[tuple[bytes, bytes]] = []
        self.tokenizedCorpus: list = []
        self.sorted_merges = {}
        self._initialize_vocabulary()

    def _initialize_training(self, input_path: str, vocab_size:int):
        self.input_path = input_path
        self.vocab_size = vocab_size
        self._read_corpus()
        self._pre_tokenize_corpus()

    def _read_corpus(self):
        with open(self.input_path, "r", encoding="utf-8") as f:
            self.corpus = f.read()

    def _get_cache_path(self, input_path, vocab_size):
        base = Path(input_path)
        cache_dir = Path("./cache")
        cache_dir.mkdir(parents=True, exist_ok=True)

        st_tag = f"st{len(self.special_tokens or [])}"
        return cache_dir / f"{base.stem}_v{vocab_size}_{st_tag}_cache.pkl"
    
    def _save(self, cache_path: Path):
        with open(cache_path, "wb") as f:
                pickle.dump({
                    "vocabulary": self.vocabulary,
                    "reverseVocab": self.reverseVocab,
                    "merges": self.merges,
                    "vocab_size": self.vocab_size,
                    "special_tokens": self.special_tokens,
                }, f)

    def _load(self, cache_path: Path):
        if not cache_path.exists():
            return False
        with open(cache_path, "rb") as f:
            data = pickle.load(f)
            self.vocabulary = data["vocabulary"]
            self.reverseVocab = data["reverseVocab"]
            self.merges = data["merges"]
            self.vocab_size = data["vocab_size"]
            self.special_tokens = data["special_tokens"]

            self.tokenizedCorpus = []
            self.corpus = None
        self.sorted_merges = {
            (a, b): i
            for i, (a, b) in enumerate(self.merges)
        }
        return True

    def _pre_tokenize_corpus(self):
        self.tokenizedCorpus = self._pre_tokenize(self.corpus)

    def _pre_tokenize(self, input_str: str):
        PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
        if type(input_str) is not str:
            raise ValueError("Input not found")
        if not self.special_tokens:
            chunks = [input_str]
            special_set = set()
        else:
            split_pat = f"({'|'.join(map(re.escape, self.special_tokens))})"
            chunks = re.split(split_pat, input_str)
            special_set = set(self.special_tokens)

        encoded_text = []
        for chunk in chunks:
            if chunk in special_set:
                token_bytes = chunk.encode("utf-8")
                token_id = self.vocabulary[token_bytes]
                encoded_text.append([token_id])
                continue
            tokens = re.findall(PAT, chunk)
            encoded_text.extend([list(tok.encode("utf-8")) for tok in tokens])

        return encoded_text

    def _initialize_vocabulary(self):
        self.vocabulary = {bytes([x]): x for x in range(256)}
        n = len(self.vocabulary)
        for token in self.special_tokens:
            self.vocabulary[token.encode("utf-8")] = n
            n+=1
        self.reverseVocab = {v: k for k, v in self.vocabulary.items()}

    def _rebuild_corpus(self, merge_pair):
        merged_bytes = self.reverseVocab[merge_pair[0]] + self.reverseVocab[merge_pair[1]]
        merged_id = self.vocabulary[merged_bytes]

        if self.num_workers > 1 and len(self.tokenizedCorpus) > 1000:
            with Pool(self.num_workers) as pool:
                new_corpus = pool.map(_rebuild_word, [(word, merge_pair, merged_id) for word in self.tokenizedCorpus])
            self.tokenizedCorpus = new_corpus
        else:
            merge_left, merge_right = merge_pair
            new_corpus = []
            for word in self.tokenizedCorpus:
                if len(word) < 2:
                    new_corpus.append(word)
                    continue

                new_word = []
                i = 0
                word_len = len(word)
                while i < word_len:
                    if i < word_len - 1 and word[i] == merge_left and word[i+1] == merge_right:
                        new_word.append(merged_id)
                        i += 2
                    else:
                        new_word.append(word[i])
                        i += 1
                new_corpus.append(new_word)
            self.tokenizedCorpus = new_corpus

    def _merge_bpe(self, input_enc: list):
        output = []
        for word in input_enc:
            word = word[:]
            while True:
                best_merge_rank = None
                best_merge_index = None
                best_merge_pair = None

                for i in range(len(word) - 1):
                    left_id = word[i]
                    right_id = word[i + 1]

                    left_bytes = self.reverseVocab[left_id]
                    right_bytes = self.reverseVocab[right_id]
                    pair = (left_bytes, right_bytes)

                    if pair not in self.sorted_merges:
                        continue

                    rank = self.sorted_merges[pair]

                    if best_merge_rank is None or rank < best_merge_rank:
                        best_merge_rank = rank
                        best_merge_index = i
                        best_merge_pair = pair

                if best_merge_pair is None:
                    break
                merged_bytes = best_merge_pair[0] + best_merge_pair[1]
                merged_pair = self.vocabulary[merged_bytes]

                word = (
                    word[:best_merge_index]
                    + [merged_pair]
                    + word[best_merge_index + 2 :]
                )

            output.append(word)
        return output
    def train_bpe(self, input_path: str, vocab_size:int):
        cache_path = self._get_cache_path(input_path, vocab_size)

        self.input_path = input_path
        self.vocab_size = vocab_size

        if self._load(cache_path):
            print(f"Loaded tokenizer from cache: {cache_path}")
            return self.reverseVocab, self.merges
        
        self._initialize_training(input_path, vocab_size)

        next_id = len(self.vocabulary)
        while True:
            if self.num_workers > 1 and len(self.tokenizedCorpus) > 1000:
                chunk_size = len(self.tokenizedCorpus) // self.num_workers
                chunks = [self.tokenizedCorpus[i:i+chunk_size] for i in range(0, len(self.tokenizedCorpus), chunk_size)]

                with Pool(self.num_workers) as pool:
                    freq_counters = pool.map(_count_pairs_chunk, chunks)
                freq = Counter()
                for counter in freq_counters:
                    freq.update(counter)
            else:
                pairs = []
                for word in self.tokenizedCorpus:
                    pairs.extend(zip(word, word[1:]))
                freq = Counter(pairs)
            candidates = (
                (pair, count)
                for pair, count in freq.items()
                if (self.reverseVocab[pair[0]] + self.reverseVocab[pair[1]]) not in self.vocabulary
            )
            if not candidates:
                break
            try:
                max_freq = max(candidates, key=lambda x: x[1])
            except ValueError:
                break
            merge_pair = max_freq[0]
            new_index = self.reverseVocab[merge_pair[0]] + self.reverseVocab[merge_pair[1]]
            if len(self.vocabulary) >= self.vocab_size:
                warnings.warn(f"Tokenizer vocabulary exceeded maximum length of {self.vocab_size} ",UserWarning)
                break
            self.vocabulary[new_index] = next_id
            self.reverseVocab[next_id] = new_index
            self.merges.append((self.reverseVocab[merge_pair[0]], self.reverseVocab[merge_pair[1]]))
            next_id += 1
            self._rebuild_corpus(max_freq[0])
        self._save(cache_path)
        return self.reverseVocab, self.merges
    
    def encode(self, input_str: str):
        tokenized_str = self._pre_tokenize(input_str)
        encoded_str = self._merge_bpe(tokenized_str)
        return encoded_str
    
    def decode(self, tokenized_input: list[int]):
        output = ""
        for word in tokenized_input:
            for token in word:
                output += self.reverseVocab[token].decode("utf-8")
        return output
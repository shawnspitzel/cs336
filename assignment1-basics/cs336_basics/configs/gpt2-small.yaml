name: atlas-llm-pretrain
project: atlas-llm-pretrain
method: grid
metric: {name: final/val, goal: maximize}

# Model Architecture
model:
  d_model: 768
  num_heads: 12
  d_ff: 3072
  num_layers: 12
  vocab_size: 50257
  context_length: 1024
  theta: 10000.0

# Training Configuration
training:
  batch_size: 64
  max_iters: 100000
  eval_interval: 500
  eval_iters: 100
  log_interval: 100
  checkpoint_interval: 1000
  seed: 42

# Optimizer Settings
optimizer:
  name: adamw  # 'sgd'
  learning_rate: 6.0e-4
  min_lr: 6.0e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  gradient_clip_norm: 1.0

# Learning Rate Schedule
lr_schedule:
  warmup_iters: 2000
  cosine_iters: 100000

data:
  train_data: data/tinystories_train_tokens.bin
  val_data: data/tinystories_val_tokens.bin
  dtype: uint16

# Checkpointing
checkpoint:
  dir: checkpoints/gpt2-small-tinystories
  resume_from: null

logging:
  use_wandb: true
  wandb_project: cs336-assignment1
  wandb_run_name: gpt2-small-tinystories-v1
  log_to_console: true

device:
  name: mps  # 'cuda', 'cpu', or 'mps'
  compile: true  # Use torch.compile